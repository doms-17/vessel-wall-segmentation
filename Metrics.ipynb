{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Elaborazione di Immagini Mediche**\n",
    "### 2021/22 - VESSELWALL SEGMENTATION CHALLENGE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*   Import delle librerie necessarie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import shutil\n",
    "import math\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import statistics\n",
    "\n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from skimage import util\n",
    "from skimage.io import imread, imshow, imsave\n",
    "\n",
    "import glob\n",
    "import pydicom\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19FoR4DwPpCc"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w2WZS_3FPos1"
   },
   "outputs": [],
   "source": [
    "def sorted_alphanum(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(data, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preparazione e creazione cartelle necessarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = ('DATASET_Intermedio','RESULTS')\n",
    "set_type = ('TRAIN','VALIDATION','TEST')\n",
    "side_type = ('_Left','_Right')\n",
    "masks_name = ('ICAL','ECAL','ICAR','ECAR')\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "train_dir = os.path.join(path,data_type[0],set_type[0])\n",
    "val_dir = os.path.join(path,data_type[0],set_type[1])\n",
    "\n",
    "newTrain_dir = os.path.join(path,data_type[1],set_type[0])\n",
    "newVal_dir = os.path.join(path,data_type[1],set_type[1])\n",
    "\n",
    "listOfPz_train = sorted_alphanum(os.listdir(train_dir))     \n",
    "listOfPz_val = sorted_alphanum(os.listdir(val_dir)) \n",
    "\n",
    "listOfPz_newTrain = sorted_alphanum(os.listdir(newTrain_dir))     \n",
    "listOfPz_newVal = sorted_alphanum(os.listdir(newVal_dir)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Metriche:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casi:\n",
      "1) Maschere sono presenti in entrambe: ottengo valore di DSC e RVD.\n",
      "2) Dice lume: None (Richiesto dalla consegna, lo calcolo solo del wall)\n",
      "3) Se non presenti le automatiche: auto_not_exists.\n",
      "4) Se non presenti le manuali: manual_not_exists.\n"
     ]
    }
   ],
   "source": [
    "#------------------------- Training Set:\n",
    "dict_train = {pz:{} for pz in listOfPz_train}\n",
    "metrics_train = {pz:{} for pz in listOfPz_train}\n",
    "info_train = {pz:[{'DSC':None}, {'RVD':None}, {'cont_Auto_not_Exists':None}, {'cont_Man_not_Exists':None}, {'Tot_Masks':None}, {'Found[%]':None}] for pz in listOfPz_train}\n",
    "\n",
    "# alloco in memoria le maschere manuali:\n",
    "for pz in listOfPz_train:\n",
    "    pi = pz.split('_')[1]\n",
    "    list_of_img_man = sorted_alphanum(os.listdir(train_dir+'/'+pz))   #Lista delle immagini e maschere per il paziente\n",
    "\n",
    "    list_of_masks_man = [img for img in list_of_img_man if 'lume' in img or 'wall' in img]\n",
    "    \n",
    "    dict_train[pz] = {mask_: imread(train_dir+'/'+pz+'/'+mask_) for mask_ in list_of_masks_man}\n",
    "    metrics_train[pz] = {mask_: [{'DSC':'auto_not_exists'}, {'RVD':'auto_not_exists'}] for mask_ in list_of_masks_man}\n",
    "# leggo man mano le maschere automatiche e confronto con le manuali allocate in memoria:\n",
    "for pz in listOfPz_newTrain:\n",
    "    pi = pz.split('_')[1]\n",
    "    list_of_img_auto = sorted_alphanum(os.listdir(newTrain_dir+'/'+pz))   #Lista delle immagini e maschere per il paziente\n",
    "\n",
    "    list_of_masks_auto = [img for img in list_of_img_auto if 'lume' in img or 'wall' in img]\n",
    "\n",
    "    for mask_ in list_of_masks_auto:\n",
    "        if mask_ in dict_train[pz].keys():    # se della maschera auto (in RESULTS) è presente anche la manuale (in DATASET_intermedio)\n",
    "            mask_auto = imread(newTrain_dir+'/'+pz+'/'+mask_).astype('bool')\n",
    "            mask_manual = dict_train[pz][mask_].astype('bool')\n",
    "            if 'wall' in mask_:\n",
    "                dice = 2*(np.sum(mask_auto*mask_manual)) / (np.sum(mask_manual) + np.sum(mask_auto))\n",
    "                # ricompongo maschere wall e lume per calcolo RVD:\n",
    "                name_pt1 = mask_.split('_')[0]\n",
    "                name_pt2 = mask_.split('_')[1]\n",
    "                name_lume = name_pt1+'_'+name_pt2+'_lume.png'\n",
    "\n",
    "                mask_auto_lume = imread(newTrain_dir+'/'+pz+'/'+name_lume).astype('bool')\n",
    "                mask_auto_lumewall = np.logical_xor(mask_auto,mask_auto_lume)\n",
    "                mask_manual_lume = dict_train[pz][name_lume].astype('bool')\n",
    "                mask_manual_lumewall = np.logical_xor(mask_manual, mask_manual_lume)\n",
    "\n",
    "                rvd_lumewall = (np.sum(mask_auto_lumewall) - np.sum(mask_manual_lumewall)) / np.sum(mask_manual_lumewall)\n",
    "                metrics_train[pz][mask_][1]['RVD'] = rvd_lumewall\n",
    "                \n",
    "            if 'lume' in mask_:\n",
    "                dice = None   # Richiesto da consegna\n",
    "                rvd_lume = (np.sum(mask_auto) - np.sum(mask_manual)) / np.sum(mask_manual)\n",
    "                metrics_train[pz][mask_][1]['RVD'] = rvd_lume\n",
    "\n",
    "            metrics_train[pz][mask_][0]['DSC'] = dice\n",
    "            \n",
    "        else:\n",
    "            metrics_train[pz][mask_] = [{'DSC':'manual_not_exists'}, {'RVD':'manual_not_exists'}]\n",
    "             \n",
    "#------------------------- Validation Set:\n",
    "dict_val = {pz:{} for pz in listOfPz_val}\n",
    "metrics_val = {pz:{} for pz in listOfPz_val}\n",
    "info_val = {pz:[{'DSC':None}, {'RVD':None}, {'cont_Auto_not_Exists':None}, {'cont_Man_not_Exists':None}, {'Tot_Masks':None}, {'Found[%]':None}] for pz in listOfPz_val}\n",
    "\n",
    "# Alloco in memoria le manuali:\n",
    "for pz in listOfPz_val:\n",
    "    pi = pz.split('_')[1]\n",
    "    list_of_img_man = sorted_alphanum(os.listdir(val_dir+'/'+pz))   #Lista delle immagini e maschere per il paziente\n",
    "\n",
    "    list_of_masks_man = [img for img in list_of_img_man if 'lume' in img or 'wall' in img]\n",
    "    \n",
    "    dict_val[pz] = {mask_: imread(val_dir+'/'+pz+'/'+mask_) for mask_ in list_of_masks_man}\n",
    "    metrics_val[pz] = {mask_: [{'DSC':'auto_not_exists'}, {'RVD':'auto_not_exists'}] for mask_ in list_of_masks_man}\n",
    "\n",
    "# Scorro man mano tra le automatiche e le paragono con le manuali allocate in memoria:\n",
    "for pz in listOfPz_newVal:\n",
    "    pi = pz.split('_')[1]\n",
    "    list_of_img_auto = sorted_alphanum(os.listdir(newVal_dir+'/'+pz))   #Lista delle immagini e maschere per il paziente\n",
    "\n",
    "    list_of_masks_auto = [img for img in list_of_img_auto if 'lume' in img or 'wall' in img]\n",
    "\n",
    "    for mask_ in list_of_masks_auto:\n",
    "        if mask_ in dict_val[pz].keys():    # se la maschera in RESULTS è presente nel DATASET intermedio\n",
    "            mask_auto = imread(newVal_dir+'/'+pz+'/'+mask_).astype('bool')\n",
    "            mask_manual = dict_val[pz][mask_].astype('bool')\n",
    "            if 'wall' in mask_:\n",
    "                dice = 2*(np.sum(mask_auto*mask_manual)) / (np.sum(mask_manual) + np.sum(mask_auto))\n",
    "                name_pt1 = mask_.split('_')[0]\n",
    "                name_pt2 = mask_.split('_')[1]\n",
    "                name_lume = name_pt1+'_'+name_pt2+'_lume.png'\n",
    "\n",
    "                mask_auto_lume = imread(newVal_dir+'/'+pz+'/'+name_lume).astype('bool')\n",
    "                mask_auto_lumewall = np.logical_xor(mask_auto,mask_auto_lume)\n",
    "                mask_manual_lume = dict_val[pz][name_lume].astype('bool')\n",
    "                mask_manual_lumewall = np.logical_xor(mask_manual, mask_manual_lume)\n",
    "\n",
    "                rvd_lumewall = (np.sum(mask_auto_lumewall) - np.sum(mask_manual_lumewall)) / np.sum(mask_manual_lumewall)\n",
    "                metrics_val[pz][mask_][1]['RVD'] = rvd_lumewall\n",
    "                \n",
    "            if 'lume' in mask_:\n",
    "                dice = None   #Richiesto da consegna\n",
    "                rvd_lume = (np.sum(mask_auto) - np.sum(mask_manual)) / np.sum(mask_manual)\n",
    "                metrics_val[pz][mask_][1]['RVD'] = rvd_lume\n",
    "\n",
    "            metrics_val[pz][mask_][0]['DSC'] = dice\n",
    "            \n",
    "        else:\n",
    "            metrics_val[pz][mask_] = [{'DSC':'manual_not_exists'}, {'RVD':'manual_not_exists'}]    # se la maschere in RESULTS non è nel DATASET intermedio\n",
    "\n",
    "print(f\"\"\"Casi:\n",
    "1) Maschere sono presenti in entrambe: ottengo valore di DSC e RVD.\n",
    "2) Dice lume: None (Richiesto dalla consegna, lo calcolo solo del wall)\n",
    "3) Se non presenti le automatiche: auto_not_exists.\n",
    "4) Se non presenti le manuali: manual_not_exists.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calcolo informazioni riassuntive per ogni paziente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------- Training Set:\n",
    "for pz in metrics_train.keys():\n",
    "    val_dsc = 0\n",
    "    val_rvd = 0\n",
    "    cont_dsc = 0\n",
    "    cont_rvd = 0\n",
    "\n",
    "    n_AnotE = 0\n",
    "    n_MnotE = 0\n",
    "\n",
    "    tot_masks = len(metrics_train[pz].keys())\n",
    "\n",
    "    for mask_ in metrics_train[pz].keys():\n",
    "        if isinstance(metrics_train[pz][mask_][0]['DSC'], (int, float)):\n",
    "            val_dsc += metrics_train[pz][mask_][0]['DSC']\n",
    "            cont_dsc += 1\n",
    "        if isinstance(metrics_train[pz][mask_][1]['RVD'], (int, float)):\n",
    "            val_rvd += metrics_train[pz][mask_][1]['RVD']\n",
    "            cont_rvd += 1\n",
    "        if metrics_train[pz][mask_][1]['RVD'] == 'auto_not_exists':\n",
    "            n_AnotE += 1\n",
    "        if metrics_train[pz][mask_][1]['RVD'] == 'manual_not_exists':\n",
    "            n_MnotE += 1\n",
    "    if cont_dsc != 0:\n",
    "        info_train[pz][0]['DSC'] = val_dsc/cont_dsc\n",
    "    if cont_rvd != 0:\n",
    "        info_train[pz][1]['RVD'] = val_rvd/cont_rvd\n",
    "    info_train[pz][2]['cont_Auto_not_Exists'] = n_AnotE\n",
    "    info_train[pz][3]['cont_Man_not_Exists'] = n_MnotE\n",
    "    info_train[pz][4]['Tot_Masks'] = tot_masks\n",
    "    if tot_masks != 0:\n",
    "        info_train[pz][5]['Found[%]'] = ((tot_masks-n_AnotE) - n_MnotE) / (tot_masks-n_AnotE) * 100\n",
    "\n",
    "#------------------------- Validation Set:\n",
    "for pz in metrics_val.keys():\n",
    "    val_dsc = 0\n",
    "    val_rvd = 0\n",
    "    cont_dsc = 0\n",
    "    cont_rvd = 0\n",
    "\n",
    "    n_AnotE = 0\n",
    "    n_MnotE = 0\n",
    "\n",
    "    tot_masks = len(metrics_val[pz].keys())\n",
    "\n",
    "    for mask_ in metrics_val[pz].keys():\n",
    "        if isinstance(metrics_val[pz][mask_][0]['DSC'], (int, float)):\n",
    "            val_dsc += metrics_val[pz][mask_][0]['DSC']\n",
    "            cont_dsc += 1\n",
    "        if isinstance(metrics_val[pz][mask_][1]['RVD'], (int, float)):\n",
    "            val_rvd += metrics_val[pz][mask_][1]['RVD']\n",
    "            cont_rvd += 1\n",
    "        if metrics_val[pz][mask_][1]['RVD'] == 'auto_not_exists':\n",
    "            n_AnotE += 1\n",
    "        if metrics_val[pz][mask_][1]['RVD'] == 'manual_not_exists':\n",
    "            n_MnotE += 1\n",
    "        \n",
    "    if cont_dsc != 0:\n",
    "        info_val[pz][0]['DSC'] = val_dsc/cont_dsc\n",
    "    if cont_rvd != 0:\n",
    "        info_val[pz][1]['RVD'] = val_rvd/cont_rvd\n",
    "    info_val[pz][2]['cont_Auto_not_Exists'] = n_AnotE\n",
    "    info_val[pz][3]['cont_Man_not_Exists'] = n_MnotE\n",
    "    info_val[pz][4]['Tot_Masks'] = tot_masks\n",
    "    if tot_masks != 0:\n",
    "        info_val[pz][5]['Found[%]'] = ((tot_masks-n_AnotE) - n_MnotE) / (tot_masks-n_AnotE) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolo DSC e RVD medi per Training e Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche finali per l'intero Training and Validation Set:\n",
      "Training Set:\n",
      "DSC [mean +- devStd]: 0.722 +- 0.044\n",
      "RVD [mean +- devStd]: 0.037 +- 0.027\n",
      "\n",
      "Validation Set:\n",
      "DSC [mean +- devStd]: 0.696 +- 0.034\n",
      "RVD [mean +- devStd]: 0.016 +- 0.043\n"
     ]
    }
   ],
   "source": [
    "#------------------------- Training Set:\n",
    "dsc_tot = []\n",
    "rvd_tot = []\n",
    "found_tot = []\n",
    "\n",
    "for pz in info_train.keys():\n",
    "    dsc, rvd, found = info_train[pz][0]['DSC'], info_train[pz][1]['RVD'], info_train[pz][5]['Found[%]']\n",
    "    if dsc != None:\n",
    "        dsc_tot.append(dsc)\n",
    "        rvd_tot.append(rvd)\n",
    "        found_tot.append(found)\n",
    "\n",
    "mean_dsc = statistics.mean(dsc_tot)\n",
    "devStd_dsc = statistics.stdev(dsc_tot)\n",
    "mean_rvd = statistics.mean(rvd_tot)\n",
    "devStd_rvd = statistics.stdev(rvd_tot)\n",
    "\n",
    "mean_found = statistics.mean(found_tot)\n",
    "\n",
    "resume_train = {'Mean-DevStd DSC': [mean_dsc,devStd_dsc], 'Mean-DevStd RVD': [mean_rvd,devStd_rvd], 'Found[%]': mean_found}\n",
    "\n",
    "print(f\"Metriche finali per l'intero Training and Validation Set:\")\n",
    "print_toSave_train = f\"\"\"Training Set:\n",
    "DSC [mean +- devStd]: {mean_dsc:.3f} +- {devStd_dsc:.3f}\n",
    "RVD [mean +- devStd]: {mean_rvd:.3f} +- {devStd_rvd:.3f}\"\"\"\n",
    "\n",
    "print(f'{print_toSave_train}')\n",
    "\n",
    "#------------------------- Validation Set:\n",
    "dsc_tot = []\n",
    "rvd_tot = []\n",
    "found_tot = []\n",
    "\n",
    "for pz in info_val.keys():\n",
    "    dsc, rvd, found = info_val[pz][0]['DSC'], info_val[pz][1]['RVD'], info_val[pz][5]['Found[%]']\n",
    "    if dsc != None:\n",
    "        dsc_tot.append(dsc)\n",
    "        rvd_tot.append(rvd)\n",
    "        found_tot.append(found)\n",
    "\n",
    "mean_dsc = statistics.mean(dsc_tot)\n",
    "devStd_dsc = statistics.stdev(dsc_tot)\n",
    "mean_rvd = statistics.mean(rvd_tot)\n",
    "devStd_rvd = statistics.stdev(rvd_tot)\n",
    "\n",
    "mean_found = statistics.mean(found_tot)\n",
    "\n",
    "resume_val = {'Mean-DevStd DSC': [mean_dsc,devStd_dsc], 'Mean-DevStd RVD': [mean_rvd,devStd_rvd], 'Found[%]': mean_found}\n",
    "\n",
    "print_toSave_val = f\"\"\"\\nValidation Set:\n",
    "DSC [mean +- devStd]: {mean_dsc:.3f} +- {devStd_dsc:.3f}\n",
    "RVD [mean +- devStd]: {mean_rvd:.3f} +- {devStd_rvd:.3f}\"\"\"\n",
    "\n",
    "print(f'{print_toSave_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Saving all metrics' information in JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------Metrics\n",
    "filename_train = 'Metrics_train.json'\n",
    "filename_val = 'Metrics_val.json'\n",
    "\n",
    "json.dump(metrics_train, open(filename_train, 'w'), indent=3)\n",
    "json.dump(metrics_val, open(filename_val, 'w'), indent=3)\n",
    "\n",
    "#------------Info \n",
    "filename_train = 'Info_train.json'\n",
    "filename_val = 'Info_val.json'\n",
    "\n",
    "json.dump([info_train,resume_train], open(filename_train, 'w'), indent=3)\n",
    "json.dump([info_val,resume_val], open(filename_val, 'w'), indent=3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
